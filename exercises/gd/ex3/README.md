This program works with two types of nodes. The first one, the root, is responsible for reading and writing files, as well as distributing the necessary information to the other nodes, the workers, who handle the calculations.

The first thing the program does is read the initial data from a file, but only with the root. Then, it sends these parameters to workers using the MPI_Bcast function. However, one of these parameters is a type defined by us, so we must also create this data type in MPI using the MPI_Type_create_struct function. To use this function, we need to specify the memory offset of each part of the type, which we do using MPI_Get_Address. Finally, MPI_Type_commit performs the commit for this data type, and then it can be used.

The trees are created on each of the nodes, and then we prepare the initial and final values of the vector with which each node will work, as well as the number of particles that will be sent to each node, with the first node (the root) getting 0, since it will not perform calculations, and the following nodes (except the last one) receiving the same number of particles, with the last node receiving a smaller remainder compared to the others to optimize the time. Then, we calculate the accelerations. Lastly, a barrier is set up to make sure all nodes start operations at the same time, with the correct positions of the particles sent to the root node.

We enter the main loop, applying the Barnes-Hut method. First, we send the particles to each worker with MPI_scatterv, who updates their velocity and position, and the information is sent back to the root with MPI_allgatherv. Since the positions of the particles have changed, the tree is regenerated, the accelerations are recalculated, and the particle information is sent to the workers again so they can compute the new velocities, after which all the information is gathered back in the root. Finally, the root node checks if the positions of the particles need to be saved to the file, and all the nodes check if it is time to finish the loop. Lastly, all memory is liberated, ussing deallocate and MPI_Type_free, the MPI process are finalized (MPI_Finalize), and a message is sent indicating that the process has completed successfully.
